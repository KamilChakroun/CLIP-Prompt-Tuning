# ðŸ”¥ Fire Detection using CLIP + Prompt Tuning

This project fine-tunes **soft prompts** for [CLIP](https://github.com/mlfoundations/open_clip) (ViT-B-32) to detect **fire vs. non-fire** images.  
Instead of retraining the whole model, we **learn prompt embeddings** that better align with your dataset, making training efficient and accurate.

---

## ðŸ“‚ Project Structure
``` bash
â”œâ”€â”€ train_clip_fire.py # Train soft prompts on fire/non-fire images
â”œâ”€â”€ predict_clip_fire.py # Run inference + save predictions
â”œâ”€â”€ learned_prompts.pth # Saved trained soft prompts (after training)
â”œâ”€â”€ fire_predictions.csv # Example output of predictions
â”œâ”€â”€ Dataset/
â”‚ â”œâ”€â”€ train/
â”‚ â”‚ â”œâ”€â”€ fire/
â”‚ â”‚ â””â”€â”€ non_fire/
â”‚ â””â”€â”€ test/
â”‚ â”‚ â”œâ”€â”€ fire/
â”‚ â”‚ â””â”€â”€ non_fire/
```
---

## âš™ï¸ How It Works

1. **Training (`train_clip_fire.py`)**
   - Loads CLIP (ViT-B-32) pretrained on 400M imageâ€“text pairs.
   - Initializes **learned prompt vectors** (random tensors).
   - Trains only the prompts (not the CLIP weights) to better separate **fire** vs **non fire**.
   - Uses **cross-entropy loss** with class balancing.
   - Saves prompts in `learned_prompts.pth`.

2. **Prediction (`predict_clip_fire.py`)**
   - Loads the trained soft prompts.
   - Encodes test images with CLIP.
   - Combines prompts + CLIP embeddings â†’ computes similarity.
   - Outputs predictions + confidence scores into a CSV.
   - Prints a **classification report** (precision, recall, F1, accuracy).

---

## ðŸš€ Usage

### 1ï¸âƒ£ Install dependencies
pip install torch torchvision pandas scikit-learn tqdm pillow open_clip_torch

### 2ï¸âƒ£ Organize dataset
``` bash
Dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ fire/       # images with fire
â”‚   â””â”€â”€ non_fire/   # images without fire
â””â”€â”€ test/
    â”œâ”€â”€ fire/
    â””â”€â”€ non_fire/
```

### 3ï¸âƒ£ Train prompts
python train_clip_fire.py

### 4ï¸âƒ£ Run inference
python predict_clip_fire.py
Results will be saved to => fire_predictions.csv
+ Console output with accuracy & classification report

### ðŸ“Š Example Output
Classification Report:

              precision    recall  f1-score   support

        fire       1.00      1.00      1.00      7000
    non fire       1.00      0.99      0.99      3500
    
    accuracy                           1.00     10500
    macro avg      1.00      1.00      1.00     10500
    weighted avg   1.00      1.00      1.00     10500

Accuracy:
âœ… Overall Accuracy: 0.9965

## ðŸ§  Key Concepts

. CLIP (Contrastive Language-Image Pretraining): Pretrained model linking images â†” text.
. Prompt Tuning: Learn small trainable vectors (â€œsoft promptsâ€) instead of retraining the full model.
. Cross-Entropy Loss: Encourages fire images to align with the â€œfireâ€ prompt, and non-fire with â€œnon fireâ€.
. Adam Optimizer: Updates prompt vectors based on gradients.

## ðŸŽ¯ Why Prompt Tuning?

âœ… Much faster than fine-tuning CLIPâ€™s full weights.
âœ… Requires fewer resources (only prompt vectors are trained).
âœ… Retains CLIPâ€™s general knowledge while adapting to fire detection.

## ðŸ™Œ Acknowledgements
- [OpenCLIP](https://github.com/mlfoundations/open_clip) for the pretrained CLIP models.  
- [Prompt Tuning: Unlocking the Potential of CLIP for Image-Text Matching](https://medium.com/@abhinavnagpal12/prompt-tuning-unlocking-the-potential-of-clip-for-image-text-matching-9dcc4772222b) â€” insightful article that helped in understanding and applying prompt tuning to this project.  

## ðŸ—ï¸ Architecture Flow

```mermaid
flowchart TD
    A[Input Image] --> B[CLIP Image Encoder]
    C[Class Names: fire, non fire] --> D[Learned Soft Prompts]
    D --> E[CLIP Text Encoder]
    B --> F[Image Features]
    E --> G[Prompt-Tuned Text Features]
    F --> H[Similarity Computation - dot product]
    G --> H
    H --> I[Prediction: fire / non fire]
```
